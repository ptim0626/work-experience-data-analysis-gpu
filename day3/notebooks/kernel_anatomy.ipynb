{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernel\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding CUDA kernels using CuPy. We will explore every component of a CUDA kernel with detailed explanations and practical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cuda availability\n",
    "print(f\"CUDA available: {cp.cuda.is_available()}\")\n",
    "print(f\"CuPy version: {cp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CUDA Kernel Structure\n",
    "\n",
    "Let's start with the simplest possible kernel and explain every component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our first cuda kernel as a string\n",
    "simple_kernel_code = r'''\n",
    "// the extern \"C\" declaration prevents C++ name mangling\n",
    "// this ensures the function name remains unchanged for CuPy to find it\n",
    "extern \"C\" {\n",
    "\n",
    "// __global__ qualifier indicates this function runs on GPU\n",
    "// and can be called from CPU (host)\n",
    "__global__ void add_constant(\n",
    "    // pointer to input array in GPU memory\n",
    "    float* input,\n",
    "    // pointer to output array in GPU memory  \n",
    "    float* output,\n",
    "    // value to add (passed by value, not pointer)\n",
    "    float constant,\n",
    "    // total number of elements to process\n",
    "    int size\n",
    ") {\n",
    "    // calculate unique thread ID across all blocks\n",
    "    // threadIdx.x: thread index within block (0 to blockDim.x-1)\n",
    "    // blockIdx.x: block index in grid (0 to gridDim.x-1)\n",
    "    // blockDim.x: number of threads per block\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    // bounds checking - essential to avoid memory errors\n",
    "    // some threads may have idx >= size due to grid/block sizing\n",
    "    if (idx < size) {\n",
    "        // each thread processes one element\n",
    "        output[idx] = input[idx] + constant;\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile the kernel\n",
    "add_constant_kernel = cp.RawKernel(simple_kernel_code, \"add_constant\")\n",
    "\n",
    "# demonstrate usage\n",
    "# data types are important\n",
    "size = cp.int32(1000)\n",
    "input_array = cp.arange(size, dtype=cp.float32)\n",
    "output_array = cp.zeros(size, dtype=cp.float32)\n",
    "constant_value = cp.float32(10.0)\n",
    "\n",
    "# calculate grid and block dimensions\n",
    "threads_per_block = 256  # common choice, multiple of 32 (warp size)\n",
    "blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "print(f\"Threads per block: {threads_per_block}\")\n",
    "print(f\"Blocks per grid: {blocks_per_grid}\")\n",
    "print(f\"Total threads: {blocks_per_grid * threads_per_block}\")\n",
    "\n",
    "# launch kernel\n",
    "# syntax: kernel((grid,), (block,), (args,))\n",
    "add_constant_kernel(\n",
    "    (blocks_per_grid,),     # grid dimensions (number of blocks)\n",
    "    (threads_per_block,),   # block dimensions (threads per block)\n",
    "    (input_array, output_array, constant_value, size)  # kernel arguments\n",
    ")\n",
    "\n",
    "# verify results\n",
    "expected = input_array + constant_value\n",
    "print(f\"\\nResults match: {cp.allclose(output_array, expected)}\")\n",
    "print(f\"Sample output: {output_array[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Functions\n",
    "Device functions are helper functions that run on GPU but can only be called from other GPU functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel with device function\n",
    "device_function_code = r'''\n",
    "extern \"C\" {\n",
    "\n",
    "// __device__ qualifier means this function:\n",
    "//   - runs on GPU\n",
    "//   - can only be called from GPU, the device (not from CPU, the host)\n",
    "// this is useful for code reuse within kernels\n",
    "__device__ float clip_value(float value, float min_val, float max_val) {\n",
    "    // ensure value is within [min_val, max_val]\n",
    "    if (value <= min_val) return min_val;\n",
    "    if (value >= max_val) return max_val;\n",
    "    return value;\n",
    "}\n",
    "\n",
    "__device__ float safe_divide(float numerator, float denominator) {\n",
    "    // avoid division by zero\n",
    "    if (denominator == 0.0f) return 0.0f;\n",
    "    return numerator / denominator;\n",
    "}\n",
    "\n",
    "__global__ void normalise_and_clip(\n",
    "    float* input,\n",
    "    float* output,\n",
    "    float min_val,\n",
    "    float max_val,\n",
    "    float scale,\n",
    "    int size\n",
    ") {\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    if (idx < size) {\n",
    "        // use device functions for clarity and reusability\n",
    "        float normalised = safe_divide(input[idx], scale);\n",
    "        output[idx] = clip_value(normalised, min_val, max_val);\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile and test\n",
    "normalise_kernel = cp.RawKernel(device_function_code, \"normalise_and_clip\")\n",
    "\n",
    "# create test data\n",
    "size = cp.int32(1000)\n",
    "input_data = cp.random.uniform(-100, 100, size).astype(cp.float32)\n",
    "output_data = cp.zeros(size, dtype=cp.float32)\n",
    "\n",
    "# parameters\n",
    "min_clip = cp.float32(-1.0)\n",
    "max_clip = cp.float32(1.0)\n",
    "scale_factor = cp.float32(50.0)\n",
    "\n",
    "# launch kernel\n",
    "threads = 256\n",
    "blocks = (size + threads - 1) // threads\n",
    "\n",
    "normalise_kernel(\n",
    "    (blocks,), (threads,),\n",
    "    (input_data, output_data, min_clip, max_clip, scale_factor, size)\n",
    ")\n",
    "\n",
    "print(f\"Input range: [{input_data.min():.2f}, {input_data.max():.2f}]\")\n",
    "print(f\"Output range: [{output_data.min():.2f}, {output_data.max():.2f}]\")\n",
    "print(f\"All outputs in [{min_clip}, {max_clip}]: \",\n",
    "      f\"{cp.all((output_data >= min_clip) & (output_data <= max_clip))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Thread Indexing for Image Processing\n",
    "\n",
    "For 2D data like images, we use 2D thread indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D kernel for image brightness adjustment\n",
    "image_kernel_code = r'''\n",
    "extern \"C\" {\n",
    "\n",
    "__global__ void adjust_brightness_2d(\n",
    "    float* image,      // input image\n",
    "    float* output,     // output image\n",
    "    float brightness,  // brightness adjustment\n",
    "    int width,         // image width\n",
    "    int height         // image height\n",
    ") {\n",
    "    // calculate 2D thread indices\n",
    "    // for x: threadIdx.x is position in block, blockIdx.x is block number\n",
    "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    // for y: threadIdx.y is position in block, blockIdx.y is block number\n",
    "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
    "    \n",
    "    // bounds checking for 2D\n",
    "    if (x < width && y < height) {\n",
    "        // convert 2D coordinates to 1D array index\n",
    "        // row-major order: index = y * width + x\n",
    "        int idx = y * width + x;\n",
    "        \n",
    "        // apply brightness adjustment\n",
    "        float new_value = image[idx] + brightness;\n",
    "        \n",
    "        // clip to valid range [0, 255] for 8-bit images\n",
    "        if (new_value < 0.0f) new_value = 0.0f;\n",
    "        if (new_value > 255.0f) new_value = 255.0f;\n",
    "        \n",
    "        output[idx] = new_value;\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile kernel\n",
    "brightness_kernel = cp.RawKernel(image_kernel_code, \"adjust_brightness_2d\")\n",
    "\n",
    "# create test image\n",
    "width, height = cp.int32(512), cp.int32(512)\n",
    "test_image = cp.random.uniform(0, 200, (height, width)).astype(cp.float32)\n",
    "output_image = cp.zeros_like(test_image)\n",
    "\n",
    "# 2D thread block configuration\n",
    "# common choice: 16x16 = 256 threads per block\n",
    "threads_x, threads_y = 16, 16\n",
    "blocks_x = (width + threads_x - 1) // threads_x\n",
    "blocks_y = (height + threads_y - 1) // threads_y\n",
    "\n",
    "print(f\"Image size: {width}x{height}\")\n",
    "print(f\"Thread block: {threads_x}x{threads_y}\")\n",
    "print(f\"Grid size: {blocks_x}x{blocks_y}\")\n",
    "\n",
    "# launch 2D kernel\n",
    "brightness_adjustment = cp.float32(50.0)\n",
    "brightness_kernel(\n",
    "    (blocks_x, blocks_y),    # 2D grid dimensions\n",
    "    (threads_x, threads_y),  # 2D block dimensions\n",
    "    (test_image, output_image, brightness_adjustment, width, height)\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal brightness: {test_image.mean():.2f}\")\n",
    "print(f\"Adjusted brightness: {output_image.mean():.2f}\")\n",
    "print(f\"Difference: {output_image.mean() - test_image.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Memory\n",
    "\n",
    "Shared memory is fast on-chip memory shared between threads in a block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel using shared memory for stencil operations\n",
    "mean_filter_code = r'''\n",
    "extern \"C\" {\n",
    "\n",
    "__global__ void mean_filter_1d_shared(\n",
    "    float* input,\n",
    "    float* output,\n",
    "    int size,\n",
    "    int radius  // filter radius (window = 2*radius + 1)\n",
    ") {\n",
    "    // declare shared memory\n",
    "    // __shared__ memory is shared between all threads in a block\n",
    "    // much faster than global memory but limited in size\n",
    "    // size must be known at compile time or allocated dynamically\n",
    "    extern __shared__ float shared_data[];\n",
    "    \n",
    "    int tid = threadIdx.x;  // thread index within block\n",
    "    int gid = threadIdx.x + blockIdx.x * blockDim.x;  // global index\n",
    "    \n",
    "    // each thread loads one element to shared memory\n",
    "    // handle boundary conditions for first and last blocks\n",
    "    if (gid < size) {\n",
    "        shared_data[tid + radius] = input[gid];\n",
    "    } else {\n",
    "        // pad with zero for threads beyond array bounds\n",
    "        shared_data[tid + radius] = 0.0f;\n",
    "    }\n",
    "\n",
    "    // load left halo elements (elements outside block needed for filter)\n",
    "    if (tid < radius) {\n",
    "        int left_idx = gid - radius;\n",
    "        if (left_idx >= 0) {\n",
    "            shared_data[tid] = input[left_idx];\n",
    "        } else {\n",
    "            // mirror boundary condition\n",
    "            shared_data[tid] = input[-left_idx];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // load right halo elements\n",
    "    if (tid >= blockDim.x - radius) {\n",
    "        int halo_offset = tid - (blockDim.x - radius);  // 0 to radius-1\n",
    "        int right_idx = blockIdx.x * blockDim.x + blockDim.x + halo_offset;\n",
    "        int shared_idx = blockDim.x + radius + halo_offset;\n",
    "        \n",
    "        if (right_idx < size) {\n",
    "            shared_data[shared_idx] = input[right_idx];\n",
    "        } else {\n",
    "            // outside the block\n",
    "            int overflow = right_idx - size + 1;\n",
    "            int mirror_idx = size - 1 - overflow;\n",
    "            if (mirror_idx < 0) mirror_idx = 0;  // ensure valid range\n",
    "            shared_data[shared_idx] = input[mirror_idx];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // synchronise threads to ensure all data is loaded\n",
    "    // all threads in block must reach this point before continuing\n",
    "    __syncthreads();\n",
    "    \n",
    "    // compute mean using shared memory\n",
    "    if (gid < size) {\n",
    "        float sum = 0.0f;\n",
    "        int count = 0;\n",
    "        \n",
    "        for (int i = -radius; i <= radius; ++i) {\n",
    "            int shared_idx = tid + radius + i;\n",
    "            // ensure we stay within shared memory bounds\n",
    "            if (shared_idx >= 0 && \n",
    "                shared_idx < blockDim.x + 2 * radius) {\n",
    "                sum += shared_data[shared_idx];\n",
    "                count++;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output[gid] = sum / count;\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile kernel\n",
    "mean_filter_kernel = cp.RawKernel(mean_filter_code, \"mean_filter_1d_shared\")\n",
    "\n",
    "# test the kernel\n",
    "size = cp.int32(10000)\n",
    "radius = cp.int32(3)\n",
    "input_signal = cp.random.normal(0, 1, size).astype(cp.float32)\n",
    "# add some noise\n",
    "input_signal += cp.random.normal(0, 0.5, size).astype(cp.float32)\n",
    "output_signal = cp.zeros_like(input_signal)\n",
    "\n",
    "# configure kernel launch\n",
    "threads = 256\n",
    "blocks = (size + threads - 1) // threads\n",
    "# calculate shared memory size needed\n",
    "shared_mem_size = (threads + 2 * radius) * 4  # 4 bytes per float\n",
    "\n",
    "print(f\"Array size: {size}\")\n",
    "print(f\"Thread block: {threads}\")\n",
    "print(f\"Grid size: {blocks}\")\n",
    "print(f\"Filter radius: {radius} (window size: {2*radius + 1})\")\n",
    "print(f\"Shared memory per block: {shared_mem_size} bytes\")\n",
    "\n",
    "# launch kernel with dynamic shared memory allocation\n",
    "mean_filter_kernel(\n",
    "    (blocks,), (threads,),\n",
    "    (input_signal, output_signal, size, radius),\n",
    "    shared_mem=shared_mem_size  # specify shared memory size\n",
    ")\n",
    "\n",
    "# compare noise levels\n",
    "input_std = cp.std(input_signal)\n",
    "output_std = cp.std(output_signal)\n",
    "print(f\"\\nInput signal std dev: {input_std:.4f}\")\n",
    "print(f\"Filtered signal std dev: {output_std:.4f}\")\n",
    "print(f\"Noise reduction: {(1 - output_std/input_std) * 100:.1f}%\")\n",
    "print(f\"\\nSample input: {input_signal[:10]}\")\n",
    "print(f\"Sample output: {output_signal[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurement\n",
    "Let see if there is any performance gain using shared memory by comparing with a naive version of 1D mean filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive version for 1D mean filter\n",
    "navie_code = r'''\n",
    "extern \"C\" {\n",
    "\n",
    "__global__ void mean_filter_1d_naive(\n",
    "    float* input,\n",
    "    float* output,\n",
    "    int size,\n",
    "    int radius\n",
    ") {\n",
    "    // global thread index\n",
    "    int gid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    float sum = 0.0f;\n",
    "    int count = 0;\n",
    "    \n",
    "    // loop through the window [-radius, radius]\n",
    "    for (int i = -radius; i <= radius; ++i) {\n",
    "        int idx = gid + i;\n",
    "        \n",
    "        // Handle boundaries consistently with shared memory version\n",
    "        if (idx < 0) {\n",
    "            // mirror at left boundary\n",
    "            idx = -idx;\n",
    "            sum += input[idx];\n",
    "            count++;\n",
    "        } else if (idx >= size) {\n",
    "            // for indices beyond size, the shared memory version pads with 0\n",
    "            // so we should add 0 (or simply skip)\n",
    "            // but still increment the count for averaging\n",
    "            count++;\n",
    "        } else {\n",
    "            // valid index\n",
    "            sum += input[idx];\n",
    "            count++;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // only write output for valid indices\n",
    "    if (gid < size && count > 0) {\n",
    "        output[gid] = sum / count;\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile kernels\n",
    "naive_kernel = cp.RawKernel(navie_code, \"mean_filter_1d_naive\")\n",
    "\n",
    "# create test data\n",
    "sizes = [cp.int32(1e4), cp.int32(1e5), cp.int32(1e6), cp.int32(1e7)]\n",
    "radius = 8\n",
    "\n",
    "print(f\"1D Mean Filter Performance Comparison\")\n",
    "print(f\"Filter radius: {radius} (window size: {2*radius + 1})\")\n",
    "print(f\"{'Size':>10} | {'Naive (ms)':>11} | {'Shared (ms)':>12} | {'Speedup':>8} | {'Match':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in sizes:\n",
    "    # create input data\n",
    "    input_data = cp.random.normal(100, 10, size).astype(cp.float32)\n",
    "    output_naive = cp.zeros(size, dtype=cp.float32)\n",
    "    output_shared = cp.zeros(size, dtype=cp.float32)\n",
    "    \n",
    "    # kernel configuration\n",
    "    threads = 256\n",
    "    blocks = (size + threads - 1) // threads\n",
    "    shared_mem_size = (threads + 2 * radius) * 4  # bytes\n",
    "    \n",
    "    # warm up\n",
    "    naive_kernel((blocks,), (threads,), \n",
    "                 (input_data, output_naive, size, radius))\n",
    "    mean_filter_kernel((blocks,), (threads,), \n",
    "                       (input_data, output_shared, size, radius), \n",
    "                       shared_mem=shared_mem_size)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # benchmark naive version\n",
    "    iterations = 1000\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        naive_kernel((blocks,), (threads,), \n",
    "                     (input_data, output_naive, size, radius))\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_naive = (time.perf_counter() - start) / iterations * 1000  # ms\n",
    "    \n",
    "    # benchmark shared memory version\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iterations):\n",
    "        mean_filter_kernel((blocks,), (threads,), \n",
    "                           (input_data, output_shared, size, radius), \n",
    "                           shared_mem=shared_mem_size)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    time_shared = (time.perf_counter() - start) / iterations * 1000  # ms\n",
    "    \n",
    "    # verify correctness\n",
    "    match = \"True\" if cp.allclose(output_naive, output_shared, rtol=1e-5) else \"False\"\n",
    "    speedup = time_naive / time_shared\n",
    "    print(f\"{size:10d} | {time_naive:11.3f} | {time_shared:12.3f} | {speedup:8.2f}x | {match:>6}\")\n",
    "\n",
    "print(f\"\\nWindow size: {2*radius + 1} elements\")\n",
    "print(f\"Naive approach: each element loaded {2*radius + 1} times\")\n",
    "print(f\"Shared approach: each element loaded once (plus halo)\")\n",
    "print(f\"Theoretical speedup upper bound: ~{2*radius + 1}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction Operations\n",
    "Reduction operations combine multiple values into one (e.g. sum, max, min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel reduction kernels\n",
    "reduction_code = r'''\n",
    "extern \"C\" {\n",
    "\n",
    "__global__ void sum_reduction(\n",
    "    float* input,\n",
    "    float* output,  // partial sums from each block\n",
    "    int size\n",
    ") {\n",
    "    // shared memory for this block's partial sum\n",
    "    extern __shared__ float sdata[];\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int gid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    // load data from global to shared memory\n",
    "    // each thread loads one element\n",
    "    if (gid < size) {\n",
    "        sdata[tid] = input[gid];\n",
    "    } else {\n",
    "        // padding for out-of-bounds\n",
    "        sdata[tid] = 0.0f;  \n",
    "    }\n",
    "    \n",
    "    // wait for all threads to load data\n",
    "    __syncthreads();\n",
    "    \n",
    "    // tree-based parallel reduction in shared memory\n",
    "    // each iteration halves the number of active threads\n",
    "    // the current implementation only works with power-of-2 blockDim.x\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n",
    "        if (tid < stride) {\n",
    "            // thread adds element that is stride positions away\n",
    "            sdata[tid] += sdata[tid + stride];\n",
    "        }\n",
    "        // ensure all threads have completed addition\n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    // thread 0 writes this block's result to global memory\n",
    "    if (tid == 0) {\n",
    "        output[blockIdx.x] = sdata[0];\n",
    "    }\n",
    "}\n",
    "\n",
    "// kernel to sum the partial results\n",
    "__global__ void final_sum(\n",
    "    float* partial_sums,\n",
    "    float* total,\n",
    "    int num_blocks\n",
    ") {\n",
    "    // simple sequential sum for small number of blocks\n",
    "    if (threadIdx.x == 0 && blockIdx.x == 0) {\n",
    "        float sum = 0.0f;\n",
    "        for (int i = 0; i < num_blocks; i++) {\n",
    "            sum += partial_sums[i];\n",
    "        }\n",
    "        *total = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "# compile kernels\n",
    "reduction_kernel = cp.RawKernel(reduction_code, \"sum_reduction\")\n",
    "final_sum_kernel = cp.RawKernel(reduction_code, \"final_sum\")\n",
    "\n",
    "# test reduction\n",
    "size = cp.int32(1e6)\n",
    "test_data = cp.ones(size, dtype=cp.float32)  # sum should equal size\n",
    "\n",
    "# configure kernel\n",
    "threads = 256\n",
    "blocks = (size + threads - 1) // threads\n",
    "partial_sums = cp.zeros(blocks, dtype=cp.float32)\n",
    "total_sum = cp.zeros(1, dtype=cp.float32)\n",
    "\n",
    "print(f\"Data size: {size}\")\n",
    "print(f\"Number of blocks: {blocks}\")\n",
    "\n",
    "# first reduction pass\n",
    "shared_mem_size = threads * 4  # 4 bytes per float\n",
    "reduction_kernel(\n",
    "    (blocks,), (threads,),\n",
    "    (test_data, partial_sums, size),\n",
    "    shared_mem=shared_mem_size\n",
    ")\n",
    "\n",
    "# final sum of partial results\n",
    "final_sum_kernel(\n",
    "    (1,), (1,),\n",
    "    (partial_sums, total_sum, cp.int32(blocks))\n",
    ")\n",
    "\n",
    "print(f\"\\nComputed sum: {total_sum[0]}\")\n",
    "print(f\"Expected sum: {size}\")\n",
    "print(f\"Error: {abs(total_sum[0] - size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
